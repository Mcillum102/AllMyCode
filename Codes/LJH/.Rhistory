library(tm)         # For text mining
library(stringr)    # For string operations
library(word2vec)   # For Word2Vec functionality
# Assuming 'ten_k_df' is a data frame containing the full 10-K text data with a 'text' column
# Step 1: Preprocess the text
preprocess_text <- function(text) {
text <- tolower(text)                          # Lowercase
text <- str_remove_all(text, "[[:punct:]]")    # Remove punctuation
unlist(str_split(text, "\\s+"))                # Tokenize text into words
}
# Apply preprocessing to each document in the 10-K dataset
ten_k_df$clean_text <- lapply(ten_k_df$text, preprocess_text)
library(tm)         # For text mining
library(stringr)    # For string operations
library(word2vec)   # For Word2Vec functionality
# Assuming 'ten_k_df' is a data frame containing the full 10-K text data with a 'text' column
# Define ten_k_df with an empty 'text' column
ten_k_df <- data.frame(text = character(), stringsAsFactors = FALSE)
# Display the structure of ten_k_df to verify
str(ten_k_df)
# Step 1: Preprocess the text
preprocess_text <- function(text) {
text <- tolower(text)                          # Lowercase
text <- str_remove_all(text, "[[:punct:]]")    # Remove punctuation
unlist(str_split(text, "\\s+"))                # Tokenize text into words
}
# Apply preprocessing to each document in the 10-K dataset
ten_k_df$clean_text <- lapply(ten_k_df$text, preprocess_text)
# Combine tokenized text into a list of sentences
sentences <- ten_k_df$clean_text
View(ten_k_df)
library(tm)         # For text mining
library(stringr)    # For string operations
library(word2vec)   # For Word2Vec functionality
# Assuming 'ten_k_df' is a data frame containing the full 10-K text data with a 'text' column
# Define ten_k_df with an empty 'text' column
ten_k_df <- data.frame(text = character(), stringsAsFactors = FALSE)
# Display the structure of ten_k_df to verify
str(ten_k_df)
# Step 1: Preprocess the text
preprocess_text <- function(text) {
text <- tolower(text)                          # Lowercase
text <- str_remove_all(text, "[[:punct:]]")    # Remove punctuation
unlist(str_split(text, "\\s+"))                # Tokenize text into words
}
# Apply preprocessing to each document in the 10-K dataset
ten_k_df$clean_text <- lapply(ten_k_df$text, preprocess_text)
# Combine tokenized text into a list of sentences
sentences <- ten_k_df$clean_text
# Convert the list of tokenized sentences into a format suitable for the word2vec package
writeLines(unlist(lapply(sentences, paste, collapse = " ")), "10k_corpus.txt")
library(tm)         # For text mining
library(stringr)    # For string operations
library(word2vec)   # For Word2Vec functionality
# Assuming 'ten_k_df' is a data frame containing the full 10-K text data with a 'text' column
# Define ten_k_df with an empty 'text' column
ten_k_df <- data.frame(text = character(), stringsAsFactors = FALSE)
# Display the structure of ten_k_df to verify
str(ten_k_df)
# Step 1: Preprocess the text
preprocess_text <- function(text) {
text <- tolower(text)                          # Lowercase
text <- str_remove_all(text, "[[:punct:]]")    # Remove punctuation
unlist(str_split(text, "\\s+"))                # Tokenize text into words
}
# Apply preprocessing to each document in the 10-K dataset
ten_k_df$clean_text <- lapply(ten_k_df$text, preprocess_text)
# Combine tokenized text into a list of sentences
sentences <- ten_k_df$clean_text
library(tm)         # For text mining
library(stringr)    # For string operations
library(word2vec)   # For Word2Vec functionality
# Assuming 'ten_k_df' is a data frame containing the full 10-K text data with a 'text' column
# Define ten_k_df with an empty 'text' column
ten_k_df <- data.frame(text = character(), stringsAsFactors = FALSE)
# Display the structure of ten_k_df to verify
str(ten_k_df)
# Step 1: Preprocess the text
preprocess_text <- function(text) {
text <- tolower(text)                          # Lowercase
text <- str_remove_all(text, "[[:punct:]]")    # Remove punctuation
unlist(str_split(text, "\\s+"))                # Tokenize text into words
}
# Apply preprocessing to each document in the 10-K dataset
ten_k_df$clean_text <- lapply(ten_k_df$text, preprocess_text)
# Combine tokenized text into a list of sentences
sentences <- ten_k_df$clean_text
# Step 2:
# Convert the list of tokenized sentences into a format suitable for the word2vec package
writeLines(unlist(lapply(sentences, paste, collapse = " ")), "10k_corpus.txt")
library(tm)         # For text mining
library(stringr)    # For string operations
library(word2vec)   # For Word2Vec functionality
# Assuming 'ten_k_df' is a data frame containing the full 10-K text data with a 'text' column
# Define ten_k_df with an empty 'text' column
ten_k_df <- data.frame(text = character(), stringsAsFactors = FALSE)
# Display the structure of ten_k_df to verify
str(ten_k_df)
# Step 1: Preprocess the text
preprocess_text <- function(text) {
text <- tolower(text)                          # Lowercase
text <- str_remove_all(text, "[[:punct:]]")    # Remove punctuation
unlist(str_split(text, "\\s+"))                # Tokenize text into words
}
# Apply preprocessing to each document in the 10-K dataset
ten_k_df$clean_text <- lapply(ten_k_df$text, preprocess_text)
# Combine tokenized text into a list of sentences
sentences <- ten_k_df$clean_text
# Step 2:
# Combine tokenized sentences into a character vector where each sentence is a single string
sentences_as_strings <- sapply(sentences, paste, collapse = " ")
# Write the sentences to a text file
writeLines(sentences_as_strings, "10k_corpus.txt")
library(tm)         # For text mining
library(stringr)    # For string operations
library(word2vec)   # For Word2Vec functionality
# Assuming 'ten_k_df' is a data frame containing the full 10-K text data with a 'text' column
# Define ten_k_df with an empty 'text' column
ten_k_df <- data.frame(text = character(), stringsAsFactors = FALSE)
# Display the structure of ten_k_df to verify
str(ten_k_df)
# Step 1: Preprocess the text
preprocess_text <- function(text) {
text <- tolower(text)                          # Lowercase
text <- str_remove_all(text, "[[:punct:]]")    # Remove punctuation
unlist(str_split(text, "\\s+"))                # Tokenize text into words
}
# Apply preprocessing to each document in the 10-K dataset
ten_k_df$clean_text <- lapply(ten_k_df$text, preprocess_text)
# Combine tokenized text into a list of sentences
sentences <- ten_k_df$clean_text
# Step 2:
# Ensure every element of 'sentences' is a character vector
sentences <- lapply(sentences, function(sentence) {
if (is.character(sentence)) {
return(sentence)
} else {
return(as.character(sentence))
}
})
# Combine tokenized sentences into a single character vector where each sentence is one string
sentences_as_strings <- sapply(sentences, function(sentence) paste(sentence, collapse = " "))
# Write the sentences to a text file
writeLines(sentences_as_strings, "10k_corpus.txt")
library(readr)
X2020_10K_item1_full <- read_csv("2020_10K_item1_full.csv")
View(X2020_10K_item1_full)
# Load necessary libraries
library(tidyverse)
# Load necessary libraries
library(tidyverse)
library(stringr)
# Load the dataset
# Assuming the dataset is located in "data/2020_10K_item1_full.csv"
ten_k_df <- read.csv("2020_10K_item1_full.csv", stringsAsFactors = FALSE)
# Display the structure of the dataset to verify the column names
str(ten_k_df)
# Preprocessing the text
# Assuming the relevant column is named "text". Replace it with the correct name if needed.
# 1. Convert text to lowercase
ten_k_df$text_clean <- str_to_lower(ten_k_df$text)
# Load necessary libraries
library(tidyverse)
library(stringr)
# Load the dataset
# Assuming the dataset is located in "data/2020_10K_item1_full.csv"
ten_k_df <- read.csv("2020_10K_item1_full.csv", stringsAsFactors = FALSE)
# Display the structure of the dataset to verify the column names
str(ten_k_df)
# Check if the 'text' column exists
if (!"text" %in% colnames(ten_k_df)) {
stop("The column 'text' does not exist in the DataFrame. Please check the column names.")
}
# Load necessary libraries
library(tidyverse)
library(stringr)
# Load the dataset
# Assuming the dataset is located in "data/2020_10K_item1_full.csv"
ten_k_df <- read.csv("data/2020_10K_item1_full.csv", stringsAsFactors = FALSE)
# Load necessary libraries
library(tidyverse)
library(stringr)
# Load the dataset
# Assuming the dataset is located in "data/2020_10K_item1_full.csv"
ten_k_df <- read.csv("2020_10K_item1_full.csv", stringsAsFactors = FALSE)
# Display the structure of the dataset to verify the column names
str(ten_k_df)
# Check if the 'item_1_text' column exists
if (!"item_1_text" %in% colnames(ten_k_df)) {
stop("The column 'item_1_text' does not exist in the DataFrame. Please check the column names.")
}
# Ensure the 'item_1_text' column is of character type
ten_k_df$item_1_text <- as.character(ten_k_df$item_1_text)
# Remove rows with missing or empty values in the 'item_1_text' column
ten_k_df <- ten_k_df %>% filter(!is.na(item_1_text) & item_1_text != "")
# Preprocessing the text
# 1. Convert text to lowercase
ten_k_df$text_clean <- str_to_lower(ten_k_df$item_1_text)
# 2. Remove punctuation
ten_k_df$text_clean <- str_replace_all(ten_k_df$text_clean, "[[:punct:]]", "")
# 3. Tokenize the text (split into words)
ten_k_df$text_tokens <- str_split(ten_k_df$text_clean, "\\s+")
# Display the cleaned dataset
head(ten_k_df)
# Optional: Save the cleaned DataFrame for later use
# write.csv(ten_k_df, "data/cleaned_10K.csv", row.names = FALSE)
# Further analysis can continue below
# Load necessary libraries
library(tidyverse)
library(stringr)
# Part 1
# Load the dataset
# Assuming the dataset is located in "data/2020_10K_item1_full.csv"
ten_k_df <- read.csv("2020_10K_item1_full.csv", stringsAsFactors = FALSE)
# Display the structure of the dataset to verify the column names
str(ten_k_df)
# Check if the 'item_1_text' column exists
if (!"item_1_text" %in% colnames(ten_k_df)) {
stop("The column 'item_1_text' does not exist in the DataFrame. Please check the column names.")
}
# Ensure the 'item_1_text' column is of character type
ten_k_df$item_1_text <- as.character(ten_k_df$item_1_text)
# Remove rows with missing or empty values in the 'item_1_text' column
ten_k_df <- ten_k_df %>% filter(!is.na(item_1_text) & item_1_text != "")
# Preprocessing the text
# 1. Convert text to lowercase
ten_k_df$text_clean <- str_to_lower(ten_k_df$item_1_text)
# 2. Remove punctuation
ten_k_df$text_clean <- str_replace_all(ten_k_df$text_clean, "[[:punct:]]", "")
# 3. Tokenize the text (split into words)
ten_k_df$text_tokens <- str_split(ten_k_df$text_clean, "\\s+")
# Part 2
# Assuming that you've generated word clouds and identified keywords manually
# Here, we'll define the chosen representative keywords for analysis
# Example chosen keywords based on manual inspection of word clouds
keywords <- c("innovation", "sustainability", "growth")
# Part 3
# Load necessary libraries
library(text2vec)  # For Word2Vec
library(data.table)  # For data manipulation
# Load the pre-trained Word2Vec model
# Replace "10k_word2vec.model" with the actual file path to your trained model
word2vec_model <- fread("10k_word2vec_vectors.csv", data.table = FALSE)
library(readr)
public_firms <- read_csv("public_firms.csv")
View(public_firms)
View(public_firms)
